import json
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Optional, Union
import torch
from io import BytesIO
import draccus
import json_numpy
import numpy as np
import torch
import uvicorn
import msgpack

from fastapi import FastAPI, Request, WebSocket, WebSocketDisconnect
from PIL import Image
from transformers import AutoTokenizer

from go1.internvl.model.go1 import GO1Model, GO1ModelConfig
from go1.internvl.train.constants import IMG_END_TOKEN
from go1.internvl.train.dataset import build_transform, dynamic_preprocess, preprocess_internvl2_5

json_numpy.patch()


def normalize(data, stats):
    """
    ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
    
    Args:
        data: éœ€è¦å½’ä¸€åŒ–çš„æ•°æ®
        stats: ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‡å€¼å’Œæ ‡å‡†å·®ï¼‰
        
    Returns:
        å½’ä¸€åŒ–åçš„æ•°æ®
    """
    return (data - stats["mean"]) / (stats["std"] + 1e-6)


def unnormalize(data, stats):
    """
    ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯å¯¹æ•°æ®è¿›è¡Œåå½’ä¸€åŒ–å¤„ç†
    
    Args:
        data: éœ€è¦åå½’ä¸€åŒ–çš„æ•°æ®
        stats: ç»Ÿè®¡ä¿¡æ¯ï¼ˆå‡å€¼å’Œæ ‡å‡†å·®ï¼‰
        
    Returns:
        åå½’ä¸€åŒ–åçš„æ•°æ®
    """
    return data * stats["std"] + stats["mean"]


def get_stats_tensor(stats_json):
    """
    ä»JSONç»Ÿè®¡ä¿¡æ¯åˆ›å»ºå¼ é‡
    
    Args:
        stats_json: JSONæ ¼å¼çš„ç»Ÿè®¡ä¿¡æ¯
        
    Returns:
        åŒ…å«ç»Ÿè®¡ä¿¡æ¯å¼ é‡çš„å­—å…¸
    """
    stats_tensor = {}

    stats_tensor["state"]={}
    stats_tensor["action"]={}

    stats_tensor["state"]["mean"] = torch.from_numpy(np.array(stats_json["observation.state"]["mean"]))
    stats_tensor["state"]["std"] = torch.from_numpy(np.array(stats_json["observation.state"]["std"]))
    stats_tensor["action"]["mean"] = torch.from_numpy(np.array(stats_json["action"]["mean"]))
    stats_tensor["action"]["std"] = torch.from_numpy(np.array(stats_json["action"]["std"]))


def multi_image_get_item(
    raw_target: Dict[str, Any],
    img_transform,
    text_tokenizer,
    num_image_token,
    cam_keys: list[str] = [
        "cam_head_color",
        "cam_hand_right_color",
        "cam_hand_left_color",
    ],
    dynamic_image_size=False,
    use_thumbnail=False,
    min_dynamic_patch=1,
    max_dynamic_patch=6,
    image_size=448,
):
    """
    å¤„ç†å¤šå›¾åƒè¾“å…¥å¹¶ç”Ÿæˆæ¨¡å‹è¾“å…¥é¡¹
    
    Args:
        raw_target: åŸå§‹ç›®æ ‡æ•°æ®ï¼ŒåŒ…å«å›¾åƒã€æ–‡æœ¬æŒ‡ä»¤ç­‰ä¿¡æ¯
        img_transform: å›¾åƒå˜æ¢å‡½æ•°ï¼Œç”¨äºå›¾åƒé¢„å¤„ç†
        text_tokenizer: æ–‡æœ¬åˆ†è¯å™¨ï¼Œç”¨äºå¤„ç†æ–‡æœ¬æ•°æ®
        num_image_token: æ¯ä¸ªå›¾åƒä½¿ç”¨çš„tokenæ•°é‡
        cam_keys: ç›¸æœºé”®åˆ—è¡¨ï¼ŒæŒ‡å®šä½¿ç”¨çš„ç›¸æœºè§†è§’
        dynamic_image_size: æ˜¯å¦ä½¿ç”¨åŠ¨æ€å›¾åƒå¤§å°
        use_thumbnail: æ˜¯å¦ä½¿ç”¨ç¼©ç•¥å›¾
        min_dynamic_patch: æœ€å°åŠ¨æ€patchæ•°
        max_dynamic_patch: æœ€å¤§åŠ¨æ€patchæ•°
        image_size: å›¾åƒå¤§å°
        
    Returns:
        åŒ…å«æ¨¡å‹è¾“å…¥çš„å­—å…¸
    """
    # åˆå§‹åŒ–å›¾åƒåˆ—è¡¨å’Œåˆ†å—ä¿¡æ¯
    images, num_tiles = [], []
    num_image = 0
    
    # éå†æ‰€æœ‰ç›¸æœºè§†è§’ï¼Œå¤„ç†å›¾åƒæ•°æ®
    for cam_key in cam_keys:
        # æ£€æŸ¥å½“å‰è§†è§’å›¾åƒæ˜¯å¦å­˜åœ¨
        if cam_key in raw_target:
            num_image += 1
            # æ ¹æ®æ˜¯å¦ä½¿ç”¨åŠ¨æ€å›¾åƒå¤§å°é€‰æ‹©å¤„ç†æ–¹å¼
            if dynamic_image_size:
                # åŠ¨æ€é¢„å¤„ç†å›¾åƒï¼Œå¯èƒ½å°†å•ä¸ªå›¾åƒåˆ†å‰²ä¸ºå¤šä¸ªpatch
                image = dynamic_preprocess(
                    raw_target[cam_key],
                    min_num=min_dynamic_patch,
                    max_num=max_dynamic_patch,
                    image_size=image_size,
                    use_thumbnail=use_thumbnail,
                )
                # å°†å¤„ç†åçš„å›¾åƒæ·»åŠ åˆ°åˆ—è¡¨ä¸­
                images += image
                # è®°å½•å½“å‰è§†è§’å›¾åƒçš„åˆ†å—æ•°é‡
                num_tiles.append(len(image))
            else:
                # ç›´æ¥æ·»åŠ åŸå§‹å›¾åƒ
                images.append(raw_target[cam_key])
                # æ¯ä¸ªè§†è§’å›¾åƒè®¡ä¸º1ä¸ªåˆ†å—
                num_tiles.append(1)

    # å¯¹æ‰€æœ‰å›¾åƒåº”ç”¨å˜æ¢å¤„ç†ï¼ˆå¦‚å½’ä¸€åŒ–ã€å°ºå¯¸è°ƒæ•´ç­‰ï¼‰
    pixel_values = [img_transform(image) for image in images]
    # å°†å›¾åƒå¼ é‡å †å ä¸ºæ‰¹æ¬¡
    pixel_values = torch.stack(pixel_values)
    # è·å–å›¾åƒå—æ€»æ•°
    num_patches = pixel_values.size(0)

    # è®¡ç®—æ¯ä¸ªè§†è§’çš„å›¾åƒtokenæ•°é‡
    num_image_tokens = [num_image_token * num_tile for num_tile in num_tiles]
    # è·å–æ–‡æœ¬ç›®æ ‡ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™ä¸ºç©ºå­—ç¬¦ä¸²ï¼‰
    ntp_target = raw_target.get("ntp_target", "")
    # æ„å»ºå¯¹è¯æ ¼å¼æ•°æ®ï¼ŒåŒ…å«äººç±»æŒ‡ä»¤å’Œæ¨¡å‹å›å¤
    conversation = [
        {"from": "human", "value": f"{'<image>'*num_image}{raw_target['final_prompt']}"},
        {"from": "gpt", "value": ntp_target},
    ]
    # ä½¿ç”¨InternVL2.5é¢„å¤„ç†å‡½æ•°å¤„ç†å¯¹è¯æ•°æ®
    ret = preprocess_internvl2_5(
        "internvl2_5",
        [conversation],
        text_tokenizer,
        num_image_tokens,
        num_image=num_image,
        group_by_length=True,
    )

    # ä¸ºæ‰“åŒ…æ•°æ®é›†è®¡ç®—position_idsï¼Œç”¨äºæ ‡è¯†æ¯ä¸ªtokenåœ¨åºåˆ—ä¸­çš„ä½ç½®
    position_ids = ret["attention_mask"].long().cumsum(-1) - 1
    position_ids.masked_fill_(ret["attention_mask"] == 0, 1)
    # è·å–å›¾åƒç»“æŸtokençš„ID
    image_end_token_id = text_tokenizer.convert_tokens_to_ids(IMG_END_TOKEN)
    # ç¡®ä¿å›¾åƒtokenæ²¡æœ‰è¢«æˆªæ–­
    assert (ret["input_ids"][0] == image_end_token_id).sum() == num_image, "image tokens are truncated"

    # åˆ›å»ºæœ€ç»ˆè¿”å›å­—å…¸ï¼ŒåŒ…å«æ¨¡å‹æ‰€éœ€çš„æ‰€æœ‰è¾“å…¥æ•°æ®
    final_ret = dict(
        # æ–‡æœ¬è¾“å…¥IDåºåˆ—
        input_ids=ret["input_ids"][0],
        # æ ‡ç­¾åºåˆ—ï¼Œç”¨äºè®­ç»ƒæ—¶è®¡ç®—æŸå¤±
        labels=ret["labels"][0],
        # æ³¨æ„åŠ›æ©ç ï¼Œæ ‡è¯†æœ‰æ•ˆtokenä½ç½®
        attention_mask=ret["attention_mask"][0],
        # ä½ç½®IDï¼Œæ ‡è¯†æ¯ä¸ªtokençš„ä½ç½®ä¿¡æ¯
        position_ids=position_ids[0],
        # å›¾åƒåƒç´ å€¼ï¼Œæ¨¡å‹è§†è§‰ç¼–ç å™¨çš„è¾“å…¥
        pixel_values=pixel_values,
        # å›¾åƒæ ‡å¿—ï¼Œæ ‡è¯†å“ªäº›è¾“å…¥æ˜¯å›¾åƒ
        image_flags=torch.tensor([1] * num_patches, dtype=torch.long),
    )
    return final_ret


class GO1Infer:
    """
    GO1æ¨¡å‹æ¨ç†ç±»ï¼Œç”¨äºåŠ è½½æ¨¡å‹å¹¶æ‰§è¡Œæ¨ç†
    """
    def __init__(
        self,
        model_path: Union[str, Path],
        data_stats_path: Union[str, Path] = None,
    ) -> Path:
        """
        åˆå§‹åŒ–GO1æ¨ç†æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            data_stats_path: æ•°æ®ç»Ÿè®¡ä¿¡æ¯è·¯å¾„
        """
        self.device = torch.device("cuda:0") if torch.cuda.is_available() else torch.device("cpu")

        self.config = GO1ModelConfig.from_pretrained(
            model_path,
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            ignore_mismatched_sizes=False,
        )
        self.image_size = self.config.force_image_size
        self.num_image_token: int = int(
            (self.image_size // self.config.vision_config.patch_size) ** 2 * (self.config.downsample_ratio**2)
        )
        self.dynamic_image_size = self.config.dynamic_image_size

        self.go1 = GO1Model.from_pretrained(model_path, config=self.config)
        self.go1.to(torch.bfloat16).to(self.device)

        self.img_transform = build_transform(
            is_train=False, input_size=self.image_size, pad2square=self.config.pad2square
        )
        self.text_tokenizer = AutoTokenizer.from_pretrained(
            model_path, add_eos_token=False, trust_remote_code=True, use_fast=False
        )

        self.norm = getattr(self.config, "norm", False)  # å¦‚æœé…ç½®ä¸­æ²¡æœ‰ norm å±æ€§ï¼Œåˆ™é»˜è®¤ä¸º False
        if self.norm:
            assert data_stats_path is not None, "data_stats_path must be provided when norm is True"
            with open(data_stats_path, "rb") as f:
                self.data_stats = get_stats_tensor(json.load(f))

    def predict_action(self, inputs: Dict[str, Any]) -> str:
        """
        é¢„æµ‹åŠ¨ä½œ
        
        Args:
            inputs: æ¨¡å‹è¾“å…¥æ•°æ®
            
        Returns:
            é¢„æµ‹çš„åŠ¨ä½œ
        """
        # print("å¼€å§‹æ¨ç†...")
        pixel_values = inputs["pixel_values"]
        input_ids = inputs["input_ids"]
        attention_mask = inputs["attention_mask"]
        position_ids = inputs["position_ids"]
        image_flags = inputs["image_flags"]
        ctrl_freqs = inputs["ctrl_freqs"]

        state = inputs["state"]
        # å¦‚æœéœ€è¦ï¼Œå¯¹çŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–
        if self.norm:
            state = normalize(state, self.data_stats["state"])

        start_time = time.time()
        device = self.device
        with torch.no_grad():
            action = self.go1(
                pixel_values=pixel_values.to(dtype=torch.bfloat16, device=device),
                input_ids=input_ids.to(device).unsqueeze(0),
                attention_mask=attention_mask.to(device).unsqueeze(0),
                position_ids=position_ids.to(device).unsqueeze(0),
                image_flags=image_flags.to(device),
                state=state.to(dtype=torch.bfloat16, device=device).unsqueeze(0),
                ctrl_freqs=ctrl_freqs.to(dtype=torch.bfloat16, device=device).unsqueeze(0),
            )
        print(f"Model inference time: {(time.time() - start_time)*1000:.3f} ms")
        outputs = action[1][0].float().cpu()

        # å¦‚æœéœ€è¦ï¼Œå¯¹åŠ¨ä½œè¿›è¡Œåå½’ä¸€åŒ–
        if self.norm:
            outputs = unnormalize(outputs, self.data_stats["action"])

        outputs = outputs.numpy()

        return outputs

    def inference(self, payload: Dict[str, Any]):
        """
        æ‰§è¡Œæ¨ç†
        
        Args:
            payload: è¾“å…¥æ•°æ®è´Ÿè½½
            éœ€è¦å¯¹è¿™ä¸ªå‡½æ•°è¿›è¡Œä¿®æ”¹å°†å›ä¼ çš„æ•°æ®é”®è¿›è¡Œä¿®æ”¹
            
        Returns:
            æ¨ç†ç»“æœ
        """
        if "base_rgb_images" in payload:
            payload["cam_head_color"] = Image.fromarray(payload['base_rgb_images'])
        if "right" in payload:
            payload["cam_hand_right_color"] = Image.fromarray(payload["right"])
        if "low_rgb_images" in payload:
            payload["cam_hand_left_color"] = Image.fromarray(payload["low_rgb_images"])

        prompt = 'pick up the big workpiece.'
        print(f"è·å–çš„æç¤º: {prompt}")
        payload["final_prompt"] = f"What action should the robot take to {prompt}?"

        inputs = multi_image_get_item(
            raw_target=payload,
            img_transform=self.img_transform,
            text_tokenizer=self.text_tokenizer,
            num_image_token=self.num_image_token,
            dynamic_image_size=self.dynamic_image_size,
            use_thumbnail=self.config.use_thumbnail,
            min_dynamic_patch=self.config.min_dynamic_patch,
            max_dynamic_patch=self.config.max_dynamic_patch,
            image_size=self.image_size,
        )

        inputs["state"] = torch.from_numpy(payload["state"]).unsqueeze(0)
        inputs["ctrl_freqs"] = torch.tensor([30])

        # for k in inputs:
        #     if torch is not None and isinstance(inputs[k], torch.Tensor):
        #         print(f"inputs[{k}] =", inputs[k].shape)

        return self.predict_action(inputs)


def to_serializable(obj: Any) -> Any:
    if isinstance(obj, np.ndarray):
        return obj.tolist()
    if isinstance(obj, np.generic):
        return obj.item()
    if isinstance(obj, dict):
        return {k: to_serializable(v) for k, v in obj.items()}
    if isinstance(obj, (list, tuple)):
        return [to_serializable(v) for v in obj]
    return obj

def decompress_image(compressed_data: bytes, format: str, expected_shape: tuple) -> np.ndarray:
    """è§£å‹ç¼©å›¾åƒæ•°æ®"""
    img = Image.open(BytesIO(compressed_data))
    
    if format == 'jpeg':
        # RGBå›¾åƒ
        img_array = np.array(img.convert('RGB'))
    elif format == 'png':
        # æ·±åº¦å›¾ (uint16)
        img_array = np.array(img)
        # ç¡®ä¿æ˜¯æ­£ç¡®çš„å½¢çŠ¶
        if len(img_array.shape) == 2:
            # å·²ç»æ˜¯2Dæ•°ç»„
            pass
        else:
            # å¯èƒ½éœ€è¦è½¬æ¢
            img_array = img_array[:, :, 0] if img_array.shape[2] > 1 else img_array.squeeze()
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ ¼å¼: {format}")
    
    return img_array


def decode_observation(obs_packed: dict) -> dict:
    """è§£ç å®¢æˆ·ç«¯å‘é€çš„observationï¼ˆæ”¯æŒå‹ç¼©æ ¼å¼ï¼‰"""

    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯å‹ç¼©æ ¼å¼ï¼ˆæ–°æ ¼å¼æœ‰'format'å­—æ®µï¼‰
        base_rgb_info = obs_packed.get(b'base_rgb_images') or obs_packed.get('base_rgb_images')
        
        if isinstance(base_rgb_info, dict) and ('format' in base_rgb_info or b'format' in base_rgb_info):
            # æ–°æ ¼å¼ï¼šå‹ç¼©çš„å›¾åƒæ•°æ®
            print("  ğŸ“¦ æ£€æµ‹åˆ°å‹ç¼©æ ¼å¼")
            
            # è§£å‹ç¼©RGBå›¾åƒ
            base_rgb = decompress_image(
                base_rgb_info.get(b'data') or base_rgb_info.get('data'),
                (base_rgb_info.get(b'format') or base_rgb_info.get('format')).decode() if isinstance(base_rgb_info.get(b'format') or base_rgb_info.get('format'), bytes) else base_rgb_info.get('format'),
                tuple(base_rgb_info.get(b'shape') or base_rgb_info.get('shape'))
            )
            
            low_rgb_info = obs_packed.get(b'low_rgb_images') or obs_packed.get('low_rgb_images')
            low_rgb = decompress_image(
                low_rgb_info.get(b'data') or low_rgb_info.get('data'),
                (low_rgb_info.get(b'format') or low_rgb_info.get('format')).decode() if isinstance(low_rgb_info.get(b'format') or low_rgb_info.get('format'), bytes) else low_rgb_info.get('format'),
                tuple(low_rgb_info.get(b'shape') or low_rgb_info.get('shape'))
            )
            
            
            print(f"  âœ“ è§£å‹å®Œæˆ - base_rgb: {base_rgb.shape}")
            
        else:
            # æ—§æ ¼å¼ï¼šåŸå§‹å­—èŠ‚æ•°æ®ï¼ˆå‘åå…¼å®¹ï¼‰
            print("  ğŸ“¦ æ£€æµ‹åˆ°åŸå§‹æ ¼å¼")
            
            # å¤„ç†æ—§æ ¼å¼çš„æ•°æ®
            base_rgb_data = base_rgb_info.get(b'data') or base_rgb_info.get('data')
            base_rgb_shape = tuple(base_rgb_info.get(b'shape') or base_rgb_info.get('shape'))
            base_rgb = np.frombuffer(base_rgb_data, dtype=np.uint8).reshape(base_rgb_shape)
            
            low_rgb_info = obs_packed.get(b'low_rgb_images') or obs_packed.get('low_rgb_images')
            low_rgb_data = low_rgb_info.get(b'data') or low_rgb_info.get('data')
            low_rgb_shape = tuple(low_rgb_info.get(b'shape') or low_rgb_info.get('shape'))
            low_rgb = np.frombuffer(low_rgb_data, dtype=np.uint8).reshape(low_rgb_shape)
        
        
        # è·å–state
        state = np.array(obs_packed.get(b'state') or obs_packed.get('state'))
        
        # æ„é€ æ ‡å‡†æ ¼å¼çš„observation
        observation_raw = {
            'state': state,
            'base_rgb_images': base_rgb,
            'low_rgb_images': low_rgb,
            'ctrl_freqs': 30,  
            'instruction': (obs_packed.get(b'instruction') or obs_packed.get('instruction'))
        }
        
        return observation_raw
        
    except Exception as e:
        print(f"âŒ è§£ç observationå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise


class GO1Server:
    def __init__(self, model_path: Union[str, Path], data_stats_path: Optional[Union[str, Path]] = None) -> None:
        self.model = GO1Infer(model_path=model_path, data_stats_path=data_stats_path)
        self.app = FastAPI(title="GO1 WS Server", version="1.0.0")
        self._register_ws()

    def _register_ws(self) -> None:
        @self.app.websocket("/ws") #å°†ä¸‹é¢çš„å‡½æ•°æ³¨å†Œä¸ºWebSocketç«¯ç‚¹
        async def ws_endpoint(websocket: WebSocket):
            await websocket.accept()  # ç­‰å¾…å¹¶æ¥å—æ¥è‡ªå®¢æˆ·ç«¯çš„WebSocketè¿æ¥è¯·æ±‚
            try:
                while True:
                    msg = await websocket.receive()
                    # ä»…æ¥å—äºŒè¿›åˆ¶å¸§
                    data_bytes: Optional[bytes] = msg.get("bytes")
                    if data_bytes is None:
                        await websocket.send_text('{"error":"expect binary msgpack frame"}')
                        continue
                    try:
                        payload = msgpack.unpackb(data_bytes, raw=False, use_list=True)  # å®¢æˆ·ç«¯æœªè®¾ç½® use_bin_type ä¹Ÿå…¼å®¹
                        print(f"æ¥æ”¶åˆ°çš„ payload keys: {list(payload.keys())}")
                    except Exception as e:
                        await websocket.send_text(f'{{"error":"msgpack unpack failed: {e}"}}')
                        continue
                    try:
                        payload = decode_observation(payload)

                    except Exception as e:
                        await websocket.send_text(f'{{"error":"reconstruct failed: {e}"}}')
                        continue
                    try:
                        result = self.model.inference(payload)
                    except Exception as e:
                        await websocket.send_text(f'{{"error":"inference failed: {e}"}}')
                        continue
                    out_bytes = msgpack.packb(to_serializable(result), use_bin_type=True)
                    await websocket.send(out_bytes)
            except WebSocketDisconnect:
                return
            except Exception as e:
                try:
                    await websocket.send_text(f'{{"error":"server error: {e}"}}')
                finally:
                    await websocket.close(code=1011)

    def run(self, host: str = "0.0.0.0", port: int = 8000, ws_max_mb: int = 64) -> None:
        # WHY: å›¾åƒå¸§è¾ƒå¤§ï¼Œæ”¾å®½ä¸Šé™ï¼ˆå•ä½å­—èŠ‚ï¼‰
        uvicorn.run(
            self.app,
            host=host,
            port=port,
            ws="websockets",
            ws_max_size=ws_max_mb * 1024 * 1024,
        )

if __name__ == "__main__":
    # GO1Server("/home/vipuser/Desktop/pick_place_go1_air_4/","/home/vipuser/Desktop/AgiBot-World/fuwei/dataset_stats.json").run(host="0.0.0.0", port=8800)
    GO1Server("/root/.cache/huggingface/hub/models--MartinB7--go1_air_pick_place_air_6/snapshots/7dc976f98a04e51816aaa4a64c0c6248dc8171ba/","/home/vipuser/Desktop/stats.json").run(host="0.0.0.0", port=8800)